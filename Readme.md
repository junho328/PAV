# ðŸ“± Visionâ€‘Language MobileÂ Agent

This repo shows an endâ€‘toâ€‘end pipeline that

1. **grabs a screenshot** from an Androidâ€¯emulator running on your laptop,  
2. **sends** it (along with a naturalâ€‘language task) to a remote **FastAPIÂ server** that hosts a Visionâ€‘LanguageÂ Model (VLM),  
3. gets back a proposed UIÂ **action** (tap, swipe, type, â€¦),  
4. and **executes** that action on the emulator via `adb`.

The two main entryâ€‘point files are:

| file | role |
|------|------|
| `<<model>>_server.py` | FastAPI server that loads **Qwenâ€‘2.5-VL** (or any other VLM) and returns an action JSON |
| `pav_client.py` | Laptopâ€‘side script: captures screenshots, calls the server, and translates the JSON into `adb` commands |

---

## 0. Prerequisites

| Machine | Requirements |
|---------|--------------|
| **Server** | â€¢ Linux with CUDAâ€‘capable GPU (24â€¯GBÂ VRAM recommended)<br>â€¢ Pythonâ€¯â‰¥â€¯3.10<br>â€¢ `torch`Â +Â `transformers`<br>â€¢ **HuggingÂ Face AccessÂ Token** (because `Qwen2.5â€‘VL` is gated) |
| **Laptop / Local PC** | â€¢ AndroidÂ Studio with a running **AVD**<br>â€¢ `adb` available in `$PATH`<br>â€¢ Pythonâ€¯â‰¥â€¯3.10 |

---

## 1. Server setup Â (`qwen_server.py`,`uitars_server.py`,`pav_qwen_server.py`)

```bash
# 1â€‘A. Create & activate venv / conda env
conda create -n agent python=3.10 -y
conda activate agent

# 1â€‘B. Install deps
pip install -r requirements.txt

# 1â€‘C. (oneâ€‘time) login to HuggingÂ Face â€“ needed for Qwenâ€‘VL
huggingface-cli login
# paste your HF accessâ€‘token (READ scope)

# 1â€‘D. Launch the server
uvicorn <server file name>:app --host 0.0.0.0 --port 8000
```

If everything loads correctly you should see:
```bash
INFO:     Uvicorn running on http://0.0.0.0:8000
Model loaded successfully.
```

## 2. Local Client setup (`pav_client.py`)
```bash
# 2â€‘A. Create venv
conda create -n agent_cli python=3.10 -y
conda activate agent_cli

# 2â€‘B. Install deps
pip install requests pillow

# 2â€‘C. Make sure `adb` is in PATH

adb devices   # should list your AVD, e.g. emulatorâ€‘5554

# ADB path set
export ANDROID_HOME=$HOME/Library/Android/sdk
export PATH=$PATH:$ANDROID_HOME/platform-tools
source ~/.zshrc # or source ~/.bashrc
```


## 3. Usage
### 3-A. Start Android emulator
- Turn on your Android studio and activate an emulator (e.g., Pixel9 API36)
- Set location to the designated place (Seoul Ai Hub)
- Login Google Account (ID:ailabsamsung@gmail.com / PW:samsung2025!)
- For Google Maps task, set the starting point should be below image.
  - <img src="assets/google_maps.png" width="200"/>
- For AliExpress task, set the starting point should be below image.
  - <img src="assets/aliexpress.png" width="200"/>
  
**You can set the starting point easily with Snapshot in Android Studio!!**

### 3-B. Run the client script
- **Don't forget that the server (`uvicorn <server file name>:app --host 0.0.0.0 --port 8000`) is already on run**
```bash
python client.py \
  --server http://<SERVER_IP>:8000/predict \
  --device_id emulator-5554 \
  --task "Please display the route to Gwanghwamun Square." \
  --image_path "qwen_3b_pav_google_screenshots/0" \
  --max_steps 10
```
